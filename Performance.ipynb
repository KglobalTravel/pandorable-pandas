{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance\n",
    "\n",
    "\n",
    "*Avoiding slow code*\n",
    "\n",
    "With pandas, you'll get the most bang for your performance-buck by avoiding antipatterns. Once you've done that there are additional options like using Numba or Cython if you really need to optimize a piece of code, but that's more work typically.\n",
    "\n",
    "This notebook will walk through several common mistakes, and show more performant ways of achieving the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mistake 1: Using pandas\n",
    "\n",
    "pandas isn't always the right choice. If you're dealing with non-tabular data, or lots of linear algebra, you might be better off using something else like Python lists / dicts / sets, or raw NumPy arrays.\n",
    "\n",
    "## Mistake 2: Using object dtype\n",
    "\n",
    "Jake VanderPlas has a [great article](https://jakevdp.github.io/blog/2014/05/09/why-python-is-slow/) on why Python is slow for many of the things we care about as analysts / scientists. One reason is the overhead that comes from  using python objects for integers, floats, etc. relative to their native versions in languages like C.\n",
    "\n",
    "As a small demonstration, we'll make two NumPy arrays, one with python integers, and one with NumPy's int64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two arrays, different dtypes\n",
    "s1 = np.arange(10000, dtype=object)\n",
    "s2 = np.arange(10000, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do a simple operation on them, like taking the sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit s1.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit s2.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy can process the specialized int64 dtype array faster than the python object version, even though they're equal. Part of this comes from the different algorithms (the NumPy version would overflow with very large integers), and part comes from the Python version having to repeated unbox the actual integer from the Python object, and re-box it for the result.\n",
    "\n",
    "Typically you would never explicitly pass in dtype=object there, but occasionally object dtypes slip into pandas\n",
    "\n",
    "* Reading messy Excel Files / CSV files\n",
    "\n",
    "  These file types either don't have or don't enforce types. pandas has to infer dtypes, which doesn't always go as expected\n",
    "\n",
    "* \"Exotic\" data types like Dates, Times, Decimals\n",
    "\n",
    "    Pandas has implemented a specialized verion of datetime.datime, and datetime.timedelta, but not datetime.date, datetime.time, Decimal, etc. Depending on your application, you might be able to treat dates as datetimess, at midnight.\n",
    "\n",
    "As discussed in the [pandas documentation](https://pandas.pydata.org/docs/user_guide/basics.html#dtypes), pandas mostly uses NumPy's data types. Recent versions include more *extension types*, which are non-NumPy dtypes inside a Series or a DataFrame. Right now the most popular are\n",
    "\n",
    "* Datetimes with Timezones\n",
    "* Categorical\n",
    "* StringDtype\n",
    "* Nullable Integer\n",
    "* Nullable Boolean\n",
    "\n",
    "Previously, pandas couldn't natively store an array of integers with some missing values, since we used `np.nan` as a missing value indicator. `np.nan` is a float, and an array of some integers and some floats is just cast to an array of floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([1, None, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have explicitly requested `dtype=object` to keep it from being cast to float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([1, None, 2], dtype=object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But as we know, object-dtype is slow. It's better to use pandas nullable integer dtype:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([1, None, 2], dtype=pd.Int64Dtype())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.Series([None] + list(range(10000)), dtype=object)\n",
    "%timeit a.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = pd.Series([None] + list(range(10000)), dtype=pd.Int64Dtype())\n",
    "%timeit b.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have some messy data in-memory (in a Python list, say) that you wish to convert, use one of pandas' parsers like\n",
    "\n",
    "- `pd.to_numeric`\n",
    "- `pd.to_datetime`\n",
    "- `pd.to_timedelta`\n",
    "\n",
    "Or use `DataFrame.convert_dtypes()` to convert from the \"old\" versions (e.g. floats with NaNs) to the new nullable versions (e.g. nullable integer).\n",
    "\n",
    "### Categoricals\n",
    "\n",
    "pandas has a [Categorical Data Type](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Categorical.html) that represents data coming from a specific, and typically *fixed* set of values. It's data model includes two components:\n",
    "\n",
    "1. `categories`: An Index storing the set of valid values.\n",
    "2. `ordered`: A boolean flag indicating whether there's an ordering between the values.\n",
    "\n",
    "If you try to set a value that's not contained in the `categories`, you'll see an exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.Categorical(['good', 'better', 'good', 'best'],\n",
    "                   categories=['good', 'better', 'best'],\n",
    "                   ordered=True)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0] = 'OK'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the primary intent of categoricals was to model data with a fixed set of allowed values, their implementation suggests another use: saving memory and improving performance.\n",
    "\n",
    "When you make a Categorical, pandas will *factorize* the values. This creates a mapping between the original value and an integer code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice two things:\n",
    "\n",
    "1. The value `\"good\"` is associated with code `0`, the first element in `.categories`.\n",
    "2. The codes are stored as `int8`.\n",
    "\n",
    "When you have many repeated values, whose regular representation are larger than int8, then storing the data as Categorical can have some performance benefits.\n",
    "\n",
    "To demonstrate this, let's suppose you had a table with every adult resident of the United States (about 321,000,000 rows) where one column stores the state abbreviation as a string.\n",
    "\n",
    "Just storing this as an object-dtype array would cost the size of the 2-character string times the 321,000,000 occurances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "population = 321_000_000\n",
    "bytes_per_person = sys.getsizeof(\"AL\")  # two characters per state\n",
    "nbytes = population * bytes_per_person\n",
    "print(\"{:,d} MB\".format(nbytes // 1_000_000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or about 16GB. How many MB would you need to store the same as a Categorical?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_bytes = 50 * sys.getsizeof(\"AL\")  # store all the states onces\n",
    "bytes_per_person = ...  # How many bytes does each row take? Which array is this? Check `.dtype.itemsize`\n",
    "# \n",
    "nbytes = (... + (population * ...))\n",
    "\n",
    "print(\"{:,d} MB\".format(nbytes // 1_00_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load solutions/performance_categorical.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So when there are many repeated values the savings can be dramatic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization\n",
    "\n",
    "Just like with NumPy, *vectorization* is key to getting good performance out of pandas.\n",
    "The short definition is \"don't do for loops\", the longer definition is \"let NumPy do the for loop in C\".\n",
    "\n",
    "As an example, let's grab some data on airports locations. We'll compote the distances between pairs of airports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports = pd.read_csv(\n",
    "    \"https://vega.github.io/vega-datasets/data/airports.csv\",\n",
    "    index_col=\"iata\",\n",
    "    nrows=500,\n",
    ")\n",
    "airports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next block is a bit of pandas magic to build a DataFrame with each pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"longitude\", \"latitude\"]\n",
    "idx = pd.MultiIndex.from_product([airports.index, airports.index],\n",
    "                                 names=['orig', 'dest'])\n",
    "\n",
    "pairs = pd.concat([\n",
    "    airports[columns]\n",
    "        .rename(columns = lambda x: f'{x}_orig')\n",
    "        .reindex(idx, level='orig'),\n",
    "    airports[columns]\n",
    "        .rename(columns = lambda x: f'{x}_dest')\n",
    "        .reindex(idx, level='dest')\n",
    "    ], axis=\"columns\"\n",
    ")\n",
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def gcd_py(lat1, lng1, lat2, lng2):\n",
    "    '''\n",
    "    Calculate great circle distance between two points.\n",
    "    https://www.johndcook.com/blog/python_longitude_latitude/\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lat1, lng1, lat2, lng2: float\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    distance:\n",
    "      distance from ``(lat1, lng1)`` to ``(lat2, lng2)`` in kilometers.\n",
    "    '''\n",
    "    degrees_to_radians = math.pi / 180.0\n",
    "    ϕ1 = (90 - lat1) * degrees_to_radians\n",
    "    ϕ2 = (90 - lat2) * degrees_to_radians\n",
    "\n",
    "    θ1 = lng1 * degrees_to_radians\n",
    "    θ2 = lng2 * degrees_to_radians\n",
    "\n",
    "    cos = (math.sin(ϕ1) * math.sin(ϕ2) * math.cos(θ1 - θ2) +\n",
    "           math.cos(ϕ1) * math.cos(ϕ2))\n",
    "    # round to avoid precision issues on identical points causing ValueErrors\n",
    "    cos = round(cos, 8)\n",
    "    arc = math.acos(cos)\n",
    "    return arc * 6373  # radius of earth, in kilometers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gcd_vec(lat1, lng1, lat2, lng2):\n",
    "    '''\n",
    "    Calculate great circle distance.\n",
    "    https://www.johndcook.com/blog/python_longitude_latitude/\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lat1, lng1, lat2, lng2: float or array of float\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    distance:\n",
    "      distance from ``(lat1, lng1)`` to ``(lat2, lng2)`` in kilometers.\n",
    "    '''\n",
    "    ϕ1 = np.deg2rad(90 - lat1)\n",
    "    ϕ2 = np.deg2rad(90 - lat2)\n",
    "\n",
    "    θ1 = np.deg2rad(lng1)\n",
    "    θ2 = np.deg2rad(lng2)\n",
    "\n",
    "    cos = (np.sin(ϕ1) * np.sin(ϕ2) * np.cos(θ1 - θ2) +\n",
    "           np.cos(ϕ1) * np.cos(ϕ2))\n",
    "    # round to avoid precision issues on identical points causing warnings\n",
    "    cos = np.round(cos, 8)\n",
    "    arc = np.arccos(cos)\n",
    "    return arc * 6373 # radius of earth, in kilometers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# gcd_py with DataFrame.apply\n",
    "r = pairs.apply(\n",
    "    lambda x: gcd_py(x['latitude_orig'],\n",
    "                     x['longitude_orig'],\n",
    "                     x['latitude_dest'],\n",
    "                     x['longitude_dest']),\n",
    "                axis=\"columns\"\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# gcd_py with manual iteration\n",
    "_ = pd.Series([gcd_py(*x) for x in pairs.itertuples(index=False)],\n",
    "              index=pairs.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# gcd_vec\n",
    "r = gcd_vec(pairs['latitude_orig'], pairs['longitude_orig'],\n",
    "            pairs['latitude_dest'], pairs['longitude_dest'])\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more, consult these pages from pandas' documentation:\n",
    "\n",
    "* Enhancing performance: https://pandas.pydata.org/docs/user_guide/enhancingperf.html\n",
    "* Scaling to larger datasets: https://pandas.pydata.org/docs/user_guide/scale.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing DataFrames\n",
    "\n",
    "pandas DataFrames are somewhat complex objects. They're a collection of many NumPy (or extension) arrays, and index and row labels. Creating them has a decent amount of overhead over a simple NumPy array or Python list.\n",
    "\n",
    "This can be important when you're collecting many similar files into a single DataFrame. Suppose we had a collection of files\n",
    "\n",
    "```\n",
    "data/\n",
    "  01.json\n",
    "  02.json\n",
    "  ...\n",
    "  99.json\n",
    "```\n",
    "\n",
    "Suppose we wanted to read all of these into a single collection. In pure Python, we would probably create an empty list and repeatedly `.append()` to it, perhaps using a list comprehension:\n",
    "\n",
    "```python\n",
    "data = []\n",
    "for path in Path(\"data\").glob(\"*.json\"):\n",
    "    with path.open() as file:\n",
    "        file_data = json.load(file)\n",
    "        data.append(file_data)\n",
    "```\n",
    "\n",
    "But with pandas, where creating or growing a DataFrame is much more expensive relative to a Python list, this is slow. Let's run some benchmarks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "size_per = 5000\n",
    "N = 100\n",
    "cols = list('abcd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timed(n=30):\n",
    "    '''\n",
    "    Running a microbenchmark.\n",
    "    '''\n",
    "    def deco(func):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            timings = []\n",
    "            for i in range(n):\n",
    "                t0 = time.time()\n",
    "                func(*args, **kwargs)\n",
    "                t1 = time.time()\n",
    "                timings.append(t1 - t0)\n",
    "            return timings\n",
    "        return wrapper\n",
    "    return deco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timed(60)\n",
    "def append_df():\n",
    "    '''\n",
    "    The pythonic (bad) way\n",
    "    '''\n",
    "    df = pd.DataFrame(columns=cols)\n",
    "    for _ in range(N):\n",
    "        df.append(pd.DataFrame(np.random.randn(size_per, 4), columns=cols))\n",
    "    return df\n",
    "\n",
    "@timed(60)\n",
    "def concat_df():\n",
    "    '''\n",
    "    The pandorabe (good) way\n",
    "    '''\n",
    "    dfs = [pd.DataFrame(np.random.randn(size_per, 4), columns=cols)\n",
    "           for _ in range(N)]\n",
    "    return pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_append = append_df()\n",
    "t_concat = concat_df()\n",
    "\n",
    "timings = (pd.DataFrame({\"Append\": t_append, \"Concat\": t_concat})\n",
    "             .stack()\n",
    "             .reset_index()\n",
    "             .rename(columns={0: 'Time (s)',\n",
    "                              'level_1': 'Method'}))\n",
    "timings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4, 6))\n",
    "sns.boxplot(x='Method', y='Time (s)', data=timings, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Numba\n",
    "\n",
    "Recent versions of pandas optionally make extensive use of Numba to speed up certain operations. This is helpful when you have some custom user-defined function that you're passing to one of pandas' `.apply`, `.agg`, or `.transform` methods (in a rolling or groupby context).\n",
    "\n",
    "Consider something like a `df.rolling(n).apply(func)`. At a high level, that operation requires\n",
    "\n",
    "1. Splitting the input into groups\n",
    "2. Applying `func` to each group\n",
    "3. Collecting the results into an output group\n",
    "\n",
    "\n",
    "<img src=\"https://docs.google.com/drawings/d/e/2PACX-1vSpZlYnXg8MfRHlRjm8JDcxkCjrQfI2XoS06JikaoRCuZiQUUgyo5yjWASU-ynNcucK2-eumooIty1-/pub?w=960&amp;h=720\">\n",
    "\n",
    "Now let's suppose we wanted to speed that up with Numba. As a user, you could `@numba.jit` your function. Depending on what your user defined function is doing, that could lead to a nice speedup. But there would still be a bunch of overhead *around* your function that would be relatively slow. Pandas would need to slice into the array (from Python), call your fast function (now in fast machine code), and jump back to Python to form the output array.\n",
    "\n",
    "<img src=\"https://docs.google.com/drawings/d/e/2PACX-1vRwvBtrV51LU2qfOxXUrggJ7h0-bTeSSozatQ7AECyhSOxEdO0ivfoXNhwWM5Q-lZvRBxmPMeAX5hzf/pub?w=960&amp;h=540\">\n",
    "\n",
    "When you use the `engine=\"numba\"` keyword, pandas and Numba able to JIT compile a lot more than just your function. We're able to JIT the entire splitting, function application, and result combination so that the whole things stays in fast machine code.\n",
    "\n",
    "<img src=\"https://docs.google.com/drawings/d/e/2PACX-1vRYpI3MI4LKZQSz2VUAxQrxiN6wAlnmTCLOF2VcYTDtF5dJEbSE6IY1MgFH8w8GH84Q2Suu9ngjgYD0/pub?w=960&amp;h=540\">\n",
    "\n",
    "For example, let's compute the mean absolute deviation. Pandas doesn't have a builtin version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mad(x):\n",
    "    return np.fabs(x - x.mean()).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = pd.DataFrame(np.random.randn(10000, 10))\n",
    "ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time ts.rolling(10).apply(mad, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time ts.rolling(10).apply(mad, engine=\"numba\", raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit ts.rolling(10).apply(mad, engine=\"numba\", raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think this this pattern, where a library accepts some user-defined function and JIT-compiles the whole operation with Numba, is gaining popularity. Libraries like [UMAP](https://umap-learn.readthedocs.io/en/latest/) allow users to pass loss functions as numba-jitted functions, enabling customizable loss functions without a loss in performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
